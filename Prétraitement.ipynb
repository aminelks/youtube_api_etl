{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf218d7",
   "metadata": {},
   "source": [
    "## Bloc consacré au cleaning du text d'un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e326df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mamin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mamin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "\n",
    "punctuations=string.punctuation.replace('@','')\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "stop.append('rt')\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93244aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    text=text.lower()\n",
    "    #Enlever la ponctuation\n",
    "    text=re.sub('[%s]' % re.escape(punctuations), '',text)\n",
    "    #Enlever les mots vides + les mentions \"@\"\n",
    "    text=\" \".join([word for word in str(text).split() if not (word in stop or word[0]==\"@\" or word[:4]==\"http\")])\n",
    "    #Word stemming\n",
    "    text=\" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    #Lemmatization\n",
    "    text=\" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175c938",
   "metadata": {},
   "source": [
    "### On récupère la BD des villes et on s'authetifie sur l'API twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510c4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1f88f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>state_name</th>\n",
       "      <th>state_code</th>\n",
       "      <th>country_id</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138883</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>233</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129652</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>DC</td>\n",
       "      <td>233</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128848</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "      <td>233</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37070</td>\n",
       "      <td>Berlin Köpenick</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>BE</td>\n",
       "      <td>82</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37069</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>BE</td>\n",
       "      <td>82</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              name            state_name state_code  country_id  \\\n",
       "0      138883     New York City              New York         NY         233   \n",
       "1      129652  Washington, D.C.  District of Columbia         DC         233   \n",
       "2      128848     San Francisco            California         CA         233   \n",
       "3       37070   Berlin Köpenick                Berlin         BE          82   \n",
       "4       37069            Berlin                Berlin         BE          82   \n",
       "\n",
       "  country_code   country_name  \n",
       "0          USA  United States  \n",
       "1          USA  United States  \n",
       "2          USA  United States  \n",
       "3           DE        Germany  \n",
       "4           DE        Germany  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country = pd.read_csv('Villes/Villes.csv')\n",
    "df_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b7323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les identifiants de l'API twitter que l'on récupere sur Twitter Developper Platform\n",
    "\n",
    "api_key = \"gMJX3zW3HN3nOgQChObsQiylc\"\n",
    "api_secret = \"u39QEZJV390xVZBGtldHPjNnkhwD0G5EQnJP4FSbN0RiLeMSQo\"\n",
    "access_token = \"1255191890653327360-YCXKJgGWUZ7AnWmFwMdqVZQkwGnd4w\"\n",
    "access_token_secret = \"KGHPuaWcOfiKJvLmNXZgL2VsObD8vox7R3APBmS45IsxU\"\n",
    "\n",
    "\n",
    "# Authetification\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f4e20",
   "metadata": {},
   "source": [
    "### Procédure de récuperation du pays du publicateur du tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f78eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_loc(loc):\n",
    "    L=re.split(',+',loc)\n",
    "    return [e.strip() for e in L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d182358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "J'ai remarqué que la structure générale de localisation est Ville, Nom of State, State Code, Pays.\n",
    "Chacun de ses éléments pouvant exister ou non:\n",
    "-Paris, France\n",
    "-Japan\n",
    "-Manhattan, NY\n",
    "Ou même quelque chose n'ayant aucun rapport: \"The streets\" par exemple.\n",
    "\n",
    "\"\"\"\n",
    "def get_Country(x):\n",
    "    if len(x)>0:\n",
    "        #Via pays\n",
    "        z=x[-1]\n",
    "        L=list(df_country[df_country['country_name']==z]['country_name'].unique())\n",
    "        if len(L)==1:\n",
    "            return L[0]\n",
    "        \n",
    "        else:\n",
    "            #Via Country Code\n",
    "            if z==\"US\":\n",
    "                z=\"USA\"\n",
    "            L=list(df_country[df_country['country_code']==z]['country_name'].unique())\n",
    "            if len(L)==1:\n",
    "                return L[0]\n",
    "            \n",
    "            else:\n",
    "                #Via State Code\n",
    "                if z==\"NYC\":\n",
    "                    z=\"NY\"\n",
    "                M=list(df_country[df_country['state_code']==z]['country_name'].unique())\n",
    "                if len(M)==1:\n",
    "                    return M[0]\n",
    "\n",
    "                else:\n",
    "                    #Via State Name\n",
    "                    M=list(df_country[df_country['state_name']==z]['country_name'].unique())\n",
    "                    if len(M)==1:\n",
    "                        return M[0]\n",
    "\n",
    "                    else:\n",
    "                        #Via Ville\n",
    "                        z=x[0]\n",
    "                        M=list(df_country[df_country['name']==z]['country_name'].unique())\n",
    "                        if len(M)==1 :\n",
    "                            return M[0]\n",
    "                        else:\n",
    "                            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33e32dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veuillez entrer le mot clée sur lequel se basera l'étude: bitcoin\n"
     ]
    }
   ],
   "source": [
    "keyword = input(\"Veuillez entrer le mot clé sur lequel se basera l'étude: \" )+\" lang:en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a88dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb38be",
   "metadata": {},
   "source": [
    "### Création de la base de donnée des Tweets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20843d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution numero : 1\n",
      "Execution numero : 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Twitter applique une restriction, de ne pas récupérer un certain nombre de tweets, on demande donc au programme en cas d'erreur de patienter\n",
    "pendant 15 minutes avant de réeffectuer des recherches.\n",
    "\"\"\"\n",
    "from time import sleep\n",
    "from afinn import Afinn\n",
    "afn = Afinn()\n",
    "limit=178\n",
    "l=0\n",
    "k=0\n",
    "for i in range(50):\n",
    "    try:\n",
    "        tweets = tweepy.Cursor(api.search_tweets, q=keyword).items(limit)\n",
    "        for tweet in tweets:\n",
    "            id=tweet.user.id_str\n",
    "            country=get_Country(split_loc(tweet.user.location))\n",
    "            text=cleaning(tweet.text)\n",
    "            sentiment_value=afn.score(text)\n",
    "            if sentiment_value > 0:\n",
    "                sentiment=\"Positif\"\n",
    "\n",
    "            elif sentiment_value < 0:\n",
    "                sentiment=\"Negatif\"\n",
    "            else:\n",
    "                sentiment=\"Neutral\"\n",
    "            tweet_list.append([id,country,text,sentiment])\n",
    "        l+=1\n",
    "        print(\"Execution numero :\",l)\n",
    "        sleep(30)\n",
    "    except:\n",
    "        k+=1\n",
    "        print(\"Sleep numero :\",k)\n",
    "        sleep(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "044b169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list=[e[0] for e in tweet_list]\n",
    "country_list=[e[1] for e in tweet_list]\n",
    "text_list=[e[2] for e in tweet_list]\n",
    "sentiment_list=[e[3] for e in tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f98618a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={'ID':id_list,\n",
    "      'Country':country_list,\n",
    "      'Text':text_list,\n",
    "      'Sentiment':sentiment_list}\n",
    "df=pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e540c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "726833b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liste des pays non disponibles dans le module pygal sur lequel on affichera une carte identifiant l'avis global du pays\n",
    "L=[\"South Sudan\",\"Nauru\",\"Tuvalu\",\"Saint Kitts And Nevis\",\"Bonaire\",\"Sint Eustatius and Saba\",\"Palau\",\"Kiribati\",\"Virgin Islands (US)\",\"Micronesia\",\"Grenada\",\"Saint Vincent And The Grenadines\",\"Solomon Islands\",\"Trinidad And Tobago\",\"Tonga\",\"Vanuatu\",\"Dominica\",\"Antigua And Barbuda\",\"Samoa\",\"Barbados\",\"Fiji Islands\",\"Saint Lucia\",\"East Timor\",\"Comoros\",\"Qatar\",\"Nicaragua\",\"The Bahamas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46d0448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.Country.isin(L)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a421c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.to_csv('Tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
